{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_data = np.load(\"data/train/sup_data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_images(images, reshape_dims=(200, 200, 1)):\n",
    "    \"\"\"Creates additional dimension for the 2D image.\"\"\"\n",
    "    return np.array([image.reshape(reshape_dims) for image in images])\n",
    "\n",
    "def separate_dataset(data, labels):\n",
    "    \"\"\"Divides the data and the corresponding labels into 3 separate datasets, each one containing just a single phase\"\"\"\n",
    "    phases = [\"ferromagnet\", \"skyrmion\", \"spiral\"]\n",
    "    separated_dataset = {}\n",
    "\n",
    "    for label, phase in enumerate(phases): # 0-ferromagnet, 1-skyrmion, 2-spiral\n",
    "        idxs = sup_data[\"labels\"] == label \n",
    "        separated_dataset[phase] = (data[idxs], labels[idxs])\n",
    "    \n",
    "    return separated_dataset\n",
    "\n",
    "def shuffle_dataset(data, labels, random_seed=42):\n",
    "    \"\"\"Suffles the samples in the dataset\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    indices = np.random.permutation(len(data))\n",
    "    return data[indices], labels[indices]\n",
    "\n",
    "\n",
    "def split_dataset(data, labels, train_ratio, dev_ratio, random_seed=42):\n",
    "    \"\"\"Splits the data into train, dev, and test sets.\"\"\"\n",
    "\n",
    "    data, labels = shuffle_dataset(data, labels, random_seed)\n",
    "    \n",
    "    train_stop = int(len(data) * train_ratio)\n",
    "    dev_stop = train_stop + int(len(data) * dev_ratio)\n",
    "\n",
    "    data_dict = {\n",
    "        \"train\": {\"images\": data[:train_stop], \"labels\": labels[:train_stop]},\n",
    "        \"dev\": {\"images\": data[train_stop:dev_stop], \"labels\": labels[train_stop:dev_stop]},\n",
    "        \"test\": {\"images\": data[dev_stop:], \"labels\": labels[dev_stop:]},\n",
    "    }\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "def train_dev_test_split(data, labels, train_ratio=0.8, dev_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits data and labels into train, dev, and test sets by aggregating results \n",
    "    from all classes in the dataset.\n",
    "    \"\"\"\n",
    "    # Initialize combined dataset structure using dictionary comprehensions\n",
    "    combined_splits = {split: {\"images\": [], \"labels\": []} for split in [\"train\", \"dev\", \"test\"]}\n",
    "    \n",
    "    # Iterate over separated data and combine splits\n",
    "    for subset_data, subset_labels in separate_dataset(data, labels).values():\n",
    "        splits = split_dataset(subset_data, subset_labels, train_ratio, dev_ratio)\n",
    "        for split in combined_splits: # train, dev, test\n",
    "            for key in combined_splits[split]: # images, labels\n",
    "                combined_splits[split][key].extend(splits[split][key])\n",
    "    \n",
    "    return combined_splits\n",
    "\n",
    "\n",
    "def save_dataset(filename, train, dev, test):\n",
    "    \"\"\"Saves the dataset into a `.npz` file.\"\"\"\n",
    "    np.savez(\n",
    "        filename,\n",
    "        train_images=train[\"images\"], train_labels=train[\"labels\"],\n",
    "        dev_images=dev[\"images\"], dev_labels=dev[\"labels\"],\n",
    "        test_images=test[\"images\"], test_labels=test[\"labels\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_images(sup_data[\"data\"])\n",
    "labels = sup_data[\"labels\"]\n",
    "\n",
    "train, dev, test = train_dev_test_split(data, labels, train_ratio=0.8, dev_ratio=0.1).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = preprocess_images(sup_data[\"data\"])\n",
    "labels = sup_data[\"labels\"]\n",
    "\n",
    "x_train, y_train, x_dev, y_dev, x_test, y_test = train_dev_test_split(data, labels, train_ratio=0.8, dev_ratio=0.1)\n",
    "\n",
    "# Save to file\n",
    "save_dataset(\n",
    "    'data/train/skyrmion_dataset2',\n",
    "    (x_train, y_train), (x_dev, y_dev), (x_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_data[\"data\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
